---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Lab Section

In this lab, we will go over regularization, classification and performance metrics. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html

# Perfomance Metrics 

## K- fold cross validatation - Resampling method

Randomly split the training data into k folds. If you specify 10 folds, then you split the data into 10 partitions. Train the model on 9 of those partitions, and test your model on the 10th partition. Iterate through until every partition has been held out. 

A smaller k is more biased, but a larger k can be very variable. 

## Bootstrapping - Resampling method

Sample with replacement. Some samples may be represented several times within the boostrap sample, while others may not be represented at all. The samples that are not selected are called out of bag samples. 

Boostrap error rates usually have less uncertainty than k-fold cross validation, but higher bias. 

## Error

Deviation of the observed value to the true value (population mean)

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$y

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## R^2
Proportion of information explained by the model. It is a measure of correlation, not accuracy. 
$$1-RSS/TSS$$ 

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$

## Sensitivity or True Positive Rate

TP = True Positives
TN = True Negatives
FP = False Positives - Type I error
FN =  False Negatives - Type II error
N = actual negative samples
P = actual positive samples

$$TPR=TP/(TP + FN)$$

## Specificity or True Negative Rate

$$TNR=TN/(TN + FP)$$

## Receiver Operating Characteristics (ROC)

Plot of True Positive Rate (sensitivity) against False Positive Rate, or plots the True Positive Rate (sensitivity) against specificity. 

Either way, a good ROC curves up through the left corner, and has a large area underneath. 

## Area under ROC curve (AUC)

The area underneath the ROC curve

## Logistic function:

$$P(X)=e^{w_0 + w_1X}/{1+e^{w_0+w_1X}}$$

\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. This includes using bootstapping, cross validation etc. to resample the training data and fit a good model.

3. Visualize if your model learned on the training data by looking at ROC curve and AUC.

4. Test how your model performs on the test data. 

### Broad steps for choosing between models according to Max Kuhn and Kjell Johnson

1. Start with several models that are the least interpretable and the most flexible, like boosted trees and svms. These models are the often the most accurate.

2. Investigate simpler models that are less opaque, like partial least squares, generalized additive models, or naive bayes models.

3. Consider using the simplest model that reasonable approximates the performance of more complex models

\newpage

```{r, include=FALSE}
library(caret)
library(ROCR)
library(pROC)
library(MASS)
library(ggplot2)
library(gridExtra)
library(plyr)
library(dplyr)
library(ggfortify)
library(glmnet)
library(tidyverse)
library(Metrics)
```

Split data into training and test set
```{r}
train_size <- floor(0.75 * nrow(airquality))
set.seed(543)
train_pos <- sample(seq_len(nrow(airquality)), size = train_size)
train_regression <- airquality[train_pos,-c(1,2)]
test_regression <- airquality[-train_pos,-c(1,2)]

dim(train_regression)
dim(test_regression)
```

## Resampling in R
```{r}
?trainControl
```

## Ridge Regression

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$
2. Create and train model 
```{r}
ctrl =  trainControl(method = "boot", 15)

Ridge_regression <- train(Temp ~ Wind + Month, data = train_regression,
                          method = 'ridge', trControl= ctrl) 
```

```{r}
Ridge_regression 
```

Examine the residuals 
```{r}
ridge_test_pred <- predict(Ridge_regression, newdata = test_regression)

#plot the predicted values vs the observed values
plot_ridge_test_pred <- data.frame(Temp_test_pred = ridge_test_pred, 
                                   Observed_Temp = test_regression$Temp)
ggplot(data = plot_ridge_test_pred) +
  geom_point(aes(x=Observed_Temp, y = Temp_test_pred)) + 
  ggtitle("True Temp Value vs Predicted Temp Value Ridge Regression") +
  theme_bw()

#median residual value should be close to zero
median(resid(Ridge_regression))
#looking at medians are good for comparing models; the smaller the residual the better
```


# Homework Due Oct. 3rd

## Lasso

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$
2. Create and train model 
```{r}
airq <- airquality
sampsize <- floor(0.75 * nrow(airq))
set.seed(543)
part <- sample(seq_len(nrow(airq)), size = sampsize)
train <- airq[part,]
test <- airq[-part,]
airqreg <- train(Month ~ Temp, train, method ='lm')


```

Examine the residuals 
```{r}
#plot the predicted values vs the observed values
predictreg <- predict(airqreg, test)
plot(predictreg, test$Month)
predtest <- cbind(predictreg, test$Month)
ggplot(data= predtest)+
  geom_point(mapping = aes(predictreg, test$Month))
#median residual value should be close to zero
resi <- resid(airqreg)
summary(resi)
```


# Classification

1. Split into training and test set 
```{r}
data(iris)

#split into training and test set 
train_size <- floor(0.75 * nrow(iris))
set.seed(543)
train_pos <- sample(seq_len(nrow(iris)), size = train_size)
train_classifier <- iris[train_pos,]
test_classifier <- iris[-train_pos,]

dim(train_classifier)
dim(test_classifier)
```


## Linear Discriminant analysis

* Good for well separated classes, more stable with small n than logistic regression, and good for more than 2 response classes. 
* LDA assumes a normal distribution with a class specific mean and common variance. 

Let's see if our data follows the assumptions of LDA. 
```{r}
slength <- ggplot(data = iris, aes(x = Sepal.Length, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25)  +
  theme_bw()
swidth <- ggplot(data = iris, aes(x = Sepal.Width, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()
plength <- ggplot(data = iris, aes(x = Petal.Length, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()
pwidth <- ggplot(data = iris, aes(x = Petal.Width, fill = Species)) + 
  geom_histogram(position="identity", alpha=0.5, bins= 25) +
  theme_bw()

grid.arrange(slength, swidth, plength, pwidth)

#
```

```{r}
LDA <- lda(Species~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
           data= train_classifier, cv= T)
#fit a model to predict the class given the parameters
```

```{r}
LDA
#lda wants the means to be different; if theyr are the same then you cant use this
```

4. Test model on test set 
```{r}
#predict the species of the test data
LDA_predict <- predict(LDA, newdata=test_classifier)
confusionMatrix(LDA_predict$class, reference = test_classifier$Species)
```

```{r}
# save the predictions in a new variable
predictions <- as.data.frame(LDA_predict$posterior) %>% 
  rownames_to_column("idx")

test_classifier <- test_classifier %>% 
  rownames_to_column("idx")

predictions_actual <- full_join(predictions,test_classifier, by = "idx" )

#r  review what the meaning of prediction_actual is

# choose the two classes we want to compare, setosa and versicolor
set_vers_true_labels <- predictions_actual %>% 
  filter(Species %in% c("setosa", "versicolor")) %>% 
  mutate(Species = as.character(Species)) 
  
#make dataframe of the prediction and the label
pred_label <- data.frame(prediction = set_vers_true_labels$setosa,
                         label = set_vers_true_labels$Species)

ggplot(pred_label, aes(x = 1:24, y = prediction, color = label))+
  geom_point()

pred <- prediction(set_vers_true_labels$setosa, set_vers_true_labels$Species, 
label.ordering = c("versicolor", "setosa")) 

perf <- performance(pred,"tpr","fpr")
plot(perf)
#every step up is correct; every step to the right is an incorrect prediction
```


## Logistic Regression

$logodds_i=B_0 + B_1X_{i1}$

Here, the log odds represents the log odds of $Y_i$ being 0 or 1. 

Where $logodds$ is the dependent variable, and $X_i$ is the independent variable. $B_{number}$ are the parameters to fit. 

Logistic Regression assumes a linear relationship between the $logodds$ and $X$.

To convert from logodds, a not intuitive quantity, to odds, a more intuitive quantity, we use this non-linear equation: 

$odds_i=e^{logodds_{i}}$
or 
$odds_i=e^{B_0 + B_1X_{i1}}$

Odds is defined as the probability that the event will occur divided by the probability that the event will not occur.

Now we convert from odds to probability.

The probability that an event will occur is the fraction of times you expect to see that event in many trials. Probabilities always range between 0 and 1.

To convert from odds to a probability, divide the odds by one plus the odds. So to convert odds of 1/9 to a probability, divide 1/9 by 10/9 to obtain the probability of 0.10

$P=odds/(odds+1)$


## Logistic Regression implementation

* Y=1 is the probability of the event occuring.
* Independent variables should not be correlated.
* Log odds and independent variables should be linearly correlated.

2. Train and fit model 
```{r}
data(iris)

#split into training and test set 
train_size <- floor(0.75 * nrow(iris))
set.seed(543)
train_pos <- sample(seq_len(nrow(iris)), size = train_size)
train_classifier <- iris[train_pos,]
test_classifier <- iris[-train_pos,]


dim(train_classifier)
dim(test_classifier)
#only look at two classes 
train_classifier_log <- train_classifier[c(which(train_classifier$Species == "setosa"),
                                           which(train_classifier$Species == "versicolor")),]
test_classifier_log <- test_classifier[c(which(test_classifier$Species == "setosa"), 
                                         which(test_classifier$Species == "versicolor")),]

train_classifier_log$Species <- factor(train_classifier_log$Species)
test_classifier_log$Species <- factor(test_classifier_log$Species)

ctrl <- trainControl(method = "repeatedcv", repeats = 15,classProbs = T,
                     savePredictions = T)

#create model. logistic regression is a bionomial general linear model. 
#predict species based on sepal length
logistic_regression <- train(Species~ Sepal.Length, data = train_classifier_log, 
                             method = "glm", family= "binomial", trControl = ctrl)
#logistic regression is part of the 'glm' (generalizing your model) family
```


```{r}
logistic_regression
```


```{r}
summary(logistic_regression)
```

3. Visualize ROC curve 
```{r}
plot(x = roc(predictor = logistic_regression$pred$setosa,
             response = logistic_regression$pred$obs)$specificities, 
     y = roc(predictor = logistic_regression$pred$setosa, 
             response = logistic_regression$pred$obs)$sensitivities,
     col= "blue", xlim = c(1, 0), type ="l", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("setosa v versicolor --", 
                                     roc(predictor = logistic_regression$pred$setosa,
                                         response = logistic_regression$pred$obs)$auc
, sep = ""), col = c("blue"), fill = c("blue"))
# an roc curve giving you an y=x graph means your model is just guessing and it's wrong
```

4. Test on an independent set
```{r}
#predict iris species using Sepal legth
logistic_regression_predict_class <- predict(logistic_regression, 
                                             newdata = test_classifier_log)

#confusion matrix
confusionMatrix(logistic_regression_predict_class, 
                reference = test_classifier_log$Species)
```

Check if log odds and independent variables are linearly correlated
```{r}
logistic_regression_predict <- predict(logistic_regression, 
                                       newdata = test_classifier_log, type = "prob")

# To convert from a probability to odds, divide the probability by one minus that probability. So if the probability is 10% or 0.10 , then the odds are 0.1/0.9 or ‘1 to 9’ 

odds_species1 <- logistic_regression_predict[,1] / (1 - logistic_regression_predict[,1])
log_odds_species1 <- log(odds_species1)
cor.test(log_odds_species1, test_classifier_log$Sepal.Length)
plot(log_odds_species1, test_classifier_log$Sepal.Length)
```

Look deeper at the logistic regression 
```{r}
logistic_predict_prob <- predict(logistic_regression,
                                 newdata = test_classifier_log, type="prob")

logistic_pred_prob_plot <- data.frame(Species_pred = logistic_predict_prob, Sepal.Length  = test_classifier_log$Sepal.Length) 

test_classifier_log$Species <- as.numeric(test_classifier_log$Species) -1

ggplot(data = test_classifier_log) +
  geom_point(aes(x=Sepal.Length, y = Species)) + 
  geom_line(data = logistic_pred_prob_plot, aes(x = Sepal.Length, 
                                                y = Species_pred.setosa, col =  "setosa"))+
  geom_line(data = logistic_pred_prob_plot, aes(x = Sepal.Length,
                                                y = Species_pred.versicolor, col = "versicolor"))+
  ggtitle("Probabilities for classifying species")
#fitted logistic regression lines
```

#Homework:

1. Use the Breast Cancer dataset from the mlbench package, and predict whether the cancer is malignant or benign using one of the algorithms we learned about in class. Give some rationale as to why you chose this algorithm. Plot ROC curves, and confusion matrices. If you are choosing a hyperparameter like K or lambda, explain how and why you chose it. 

Look out for the fact that the data IS NOT CLEAN



try making a correlation graph

```{r}

data <- BreastCancer %>%
  na.omit() 
data$Bl.cromatin <- as.numeric(data$Bl.cromatin)
data$Cl.thickness <- as.numeric(data$Cl.thickness)
data$Cell.size <- as.numeric(data$Cell.size)
data$Cell.shape <- as.numeric(data$Cell.shape)
data$Marg.adhesion <- as.numeric(data$Marg.adhesion)
data$Epith.c.size <- as.numeric(data$Epith.c.size)
data$Bare.nuclei <- as.numeric(data$Bare.nuclei)
data$Normal.nucleoli <- as.numeric(data$Normal.nucleoli)
data$Mitoses <- as.numeric(data$Mitoses)

class(data$Bl.cromatin)
#libra
samp <- floor(0.75 * nrow(data))
part <- sample(seq(nrow(data)), size = samp)
train.bc <- data[part,]
test.bc <- data[-part,]
#train
```

Logistic Regression and K-Fold Cross Validation
```{r}
##ctrl <- trainControl()
## <- train(, data = , method = "", trControl = ctrl)
ctrl <- trainControl(method = "repeatedcv", repeats = 25,classProbs = T,
                     savePredictions = T)
breastreg <- train(Class ~ Bl.cromatin, data, method = "glm", family= "binomial", trControl = ctrl)
predict.bc <- predict(breastreg, newdata = test.bc)
k.conf <- confusionMatrix(predict.bc, reference = test.bc$Class)

#breastlm <- lm()
#test
```


Linear Discriminant Analysis:
it doesnt follow assumptions
```{r}
# Check assumptions of LDA

blcromatin <- ggplot(data = data, aes(x = Bl.cromatin, fill = Class)) + 
  stat_count(width = 0.5)  +
  theme_bw()

nucleoli <- ggplot(data = data, aes(x = Normal.nucleoli, fill = Class)) + 
  stat_count(width = 0.5)  +
  theme_bw()

nuclei <- ggplot(data = data, aes(x = Bare.nuclei, fill = Class)) + 
  stat_count(width = 0.5)  +
  theme_bw()

adhesion <- ggplot(data = data, aes(x = Marg.adhesion, fill = Class)) + 
  stat_count(width = 0.5)  +
  theme_bw()

grid.arrange(blcromatin,nucleoli,nuclei, adhesion)

# test for normality

shapiro.test(data$Bl.cromatin)
shapiro.test(data$Normal.nucleoli)
shapiro.test(data$Bare.nuclei)
shapiro.test(data$Marg.adhesion)

## predict(, newdata="")
lda.bc <- lda(Class ~ Bl.cromatin + Normal.nucleoli + Bare.nuclei + Marg.adhesion, 
              data = train.bc, cv = T)
lda.predict <- predict(lda.bc, newdata = test.bc)
lda.conf <- confusionMatrix(lda.predict$class, reference = test.bc$Class)
lda.predict
```

Logistic Regression with Bootstrapping
```{r}
ctrl2 <- trainControl(method = "boot", classProbs = T,
                     savePredictions = T)
bcreg.boot <- train(Class ~ Bl.cromatin, data, method = "glm", family= "binomial", trControl = ctrl2)
predict.boot <- predict(bcreg.boot, newdata = test.bc)
boot.conf <- confusionMatrix(bcreg.boot, reference = test.bc$Class)

```


ROC Curve
```{r}
roc.lda <- roc(predictor = lda.predict$posterior[,2], response = test.bc$Class)
roc.kfold <- roc(predictor = breastreg$pred$benign, response =breastreg$pred$obs)
roc.boot <- roc(predictor = bcreg.boot$pred$benign, response = bcreg.boot$pred$obs)

plot(x = roc.lda$specificities, 
     y = roc.lda$sensitivities,
     col= "red", xlim = c(1, 0), type ="l", main = "Linear Discriminant Analysis ROC curve",ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("Benign vs Malignant --", 
                                     roc.lda$auc
, sep = ""), col = c("red"), fill = c("red"))

plot(x = roc.kfold$specificities, 
     y = roc.kfold$sensitivities,
     col= "green", xlim = c(1, 0), type ="l", main = "Logistic Regression K-Fold ROC curve",ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("Benign vs Malignant --", 
                                     roc.kfold$auc
, sep = ""), col = c("green"), fill = c("green"))

plot(x = roc.boot$specificities, 
     y = roc.boot$sensitivities,
     col= "orange", xlim = c(1, 0), type ="l", main = "Logistic Regression with Bootstrapping ROC curve", ylab = "Sensitivity",
     xlab = "Specificity")
legend("bottomright", legend = paste("Benign vs Malignant --", 
                                     roc.boot$auc
, sep = ""), col = c("orange"), fill = c("orange"))


```

For this assignment, I chose to analyze the data by three different models: logistic regresion with k-fold cross validation, Linear Discriminant Analysis and logistic regression with bootstrapping. For the k-fold cross validation, I chose a K of 25. This value was arbitraryly chosed. An LDA was ran however, the data did not fit any of the assumptions of LDA. Lastly, a logistic regression with bootstrapping was ran so that the values chosen would better mimic the actual population. 

Although the LDA had the highest accuracy (95.9%), this test could not be accepted due to the fact that the data did not have a normal distribution, nor did it have class specific means. Both logistic regression has similar accuracy levels (k-fold: 90.1, bootstrapping: 90.0 ), the k-fold model gave a slightly better result in predicting classes. 




References: 
https://sebastianraschka.com/Articles/2014_python_lda.html

https://towardsdatascience.com/building-a-multiple-linear-regression-model-and-assumptions-of-linear-regression-a-z-9769a6a0de42

http://www.statisticssolutions.com/wp-content/uploads/wp-post-to-pdf-enhanced-cache/1/assumptions-of-logistic-regression.pdf

https://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/  , https://sebastianraschka.com/Articles/2014_python_lda.html


Other cool sites: 
https://www.countbayesie.com/blog/2019/6/12/logistic-regression-from-bayes-theorem
https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/